services:
  llm:
    build: ./llm
    container_name: llm
    ports:
      - "11434:11434"
    networks:
      - odin_network
    volumes:
      - ~/projects/odin_mind/llm/models:/app/llm/models
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_DEBUG=false
    restart: unless-stopped

  ui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ui
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_URL=http://llm:11434
    networks:
      - odin_network
    depends_on:
      - llm
    restart: unless-stopped

  letta:
    image: letta/letta:latest
    container_name: letta
    ports:
      - "7861:7861"
    networks:
      - odin_network
    depends_on:
      - llm
    restart: unless-stopped

networks:
  odin_network:
    driver: bridge
    name: odin_network  # Explicitly name the network to avoid prefixing issues
